# Certified Kubernetes Administrator (CKA)

## Intro
Test is hands on - no multiple choice  

Update coming 2019 1st QTR for version 1.12.0  

Download the handbook from github  

Objectives: 
- Scheduling
- logging and monitoring
- application lifecycle management
- cluster, security, storage, troubleshooting, core concepts, networking, installation, configuration and validation.  

#### Cloud Playground (command to join nodes to cluster)
`kubeadm join 172.31.108.193:6443 --token 866cng.ure2k193xwxvmgl0 --discovery-token-ca-cert-hash sha256:b15c34c7c169cdfe6ca8f0bc385401dd1db7f57fa95e5b04eac39939948bec58`  

## Bare Image Install on CentOS

    sudo swapoff -a 
    sudo vi /etc/fstab # comment out #/root/swap

Install and config Docker

    sudo yum -y install docker
    sudo systemctl enable docker
    sudo systemctl start docker

Add Kubernetes repo  

    cat << EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
    [kubernetes]
    name=Kubernetes
    baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
    enabled=1
    gpgcheck=1
    repo_gpgcheck=1
    gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
    EOF

Turn of selinux

    sudo setenforce 0
    sudo vi /etc/selinux

Change the line that says `SELINUX=enforcing` to `SELINUX=permissive` and save the file.

Install Kubernetes Components

    sudo yum install -y kubelet kubeadm kubectl
    sudo systemctl enable kubelet
    sudo systemctl start kubelet

Configure sysctl  

    cat << EOF | sudo tee /etc/sysctl.d/k8s.conf
    net.bridge.bridge-nf-call-ip6tables = 1
    net.bridge.bridge-nf-call-iptables = 1
    EOF
    
    sudo sysctl --system

Initialize the Kube Master. Do this only on the master node

    sudo kubeadm init --pod-network-cidr=10.244.0.0/16
    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config

Install flannel networking

    kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml

The `kubeadm init` command that you ran on the master should output a `kubeadm join` command containing a token and hash. You will need to copy that command from the master and run it on all worker nodes with `sudo`.

    sudo kubeadm join $controller_private_ip:6443 --token $token --discovery-token-ca-cert-hash $hash

Now you are ready to verify that the cluster is up and running. On the Kube Master server, check the list of nodes.

    kubectl get nodes

It should look something like this:

    NAME                      STATUS   ROLES    AGE     VERSION
    wboyd4c.mylabserver.com   Ready    master   3m36s   v1.12.2
    wboyd5c.mylabserver.com   Ready    <none>   23s     v1.12.2

Make sure that all of your nodes are listed and that all have a STATUS of `Ready`.

### Kubernetes API Primitives and Cluster Architecture

Records of intent - ensures object will exist, number of instances, restart descriptions, etc..

Contains Object Spec and Object Status

Spec: is our desired state

Status: is the actual state of the object

**Kubernetes YAML**

JSON converted from YAML file

Common Kubernetes Objects

- Nodes
- Pods
- Deployments
- Services
- ConfigMaps

**Names and UIDs**

Names:
- all have unique names
- max length of 253 chars
- dash - and period . allowed

UIDs:
- all objectshave unique uid
- generated by k8s
- spatially and temporarily unique

Namespaces:
- multiple virtual clustrers
- large deployments usually
- provide scopes
- same policy
- kube-system namespace for system pods
- allow for resource quotas

Nodes:
- any worker machine
- may  be VM or physical
- managed by master
- container runtime, kublet, kubeproxy
- not inherently created by k8s, but by cloud providers
- k8s checks node for validity

Cloud Controller Managers:
- route controller (gce clusters only)
- service controller
    - listens for service creation calls, lb, etc..
- PersistantVolumeLabels controller
    - applies labels on AWS EBS and GCE Disks (needs labels)

Node Controller:
- CIDR block assignment
- Keeps tracks of nodes
- Monitors the node health
- Evicts pods if unhealthy 
- Can taint nodes based on conditions

Note: you should spread nodes across AZs

### Kubernetes Services and Network Primitives
**Kubernetes Services**
Pods:
- are the simplest k8s object, one or more containers running on a single node
- Ephemeral, disposable and replacable --stateless
- Not usually managed directly

Deployments:
- Deployment specs
	- image to run
	- number of replicas
Services:
- Deployment of exposed services
- what port is exposed - load balancing and port forwarding types

If pod network IP method: deployment has a single IP
- kubeproxy handles the load balancing - directs traffic

___Kubernetes is both imperative and declarative___

**YAML**
Contain:
apiVersion:
kind:
metadata:
	name: (define the namespace, or blank = default)

Ingress:
- load balancers and ingress controllers

Jobs are applications to run inside pods. Good for batch processing
Running Jobs:
`kubectl create -f <filename.yaml>`

Running Pods:
`kubectl apply -f <filename.yaml>`

### Other Deployments
- Minikube Single node k8s cluster on local workstation
- Kubeadm - multi-node localy, but challenging
	- Would need own CNI (Cluster Network Interface)
- Ubuntu on LXD 9 cluster host on local machine
- Of course, google, azure, AWS, IBM, etc...
- Turnkey solutions are available on cloud providers
### Add-on
- CNI - calico, l3 networking
- Canal - unites Flanel and Calico
- Contiv - native L3 BGP (open source)
- Flannel - Overlay network provider
- Multus - multi plugin for multiple network support
- NSX-T integrates VMWare NXT

#### Cluster Management
`kubectl get pods`  
`kubectl describe pods`  
`kubectl get nodes`  
`kubectl describe nodes`  
`kubectl describe <node|pod|etc> <name>`  
`kubectl get pods --namespace=kube-system`  
